{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import json \n",
    "from shapely.geometry import shape, Point\n",
    "import shapely\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import folium\n",
    "\n",
    "from vincenty import vincenty\n",
    "\n",
    "import requests\n",
    "import urllib.parse\n",
    "import re\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Option d'affchage\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation de la base\n",
    "df0 = pd.read_csv('data/valeursfoncieres-2019.txt', sep = '|')\n",
    "\n",
    "# Copie de la base\n",
    "df1 = df0.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Nettoyage de la base de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Découverte de la base\n",
    "\n",
    "df1.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.1. Séléction des lignes et colonnes intéressantes pour le projet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sélection des colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des différentes colonnes\n",
    "\n",
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélection des colonnes\n",
    "\n",
    "df = df1[[\n",
    "        'Date mutation', 'Nature mutation', 'Valeur fonciere',\n",
    "        'No voie', 'Type de voie', 'Code voie', 'Voie', 'Code postal',\n",
    "        'Commune', 'Code departement', 'Code commune', 'Type local',\n",
    "        'Surface reelle bati', 'Nombre pieces principales', 'Surface terrain',\n",
    "        'Section', 'No plan'\n",
    "        ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sélection des lignes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Premier tri sur les lignes \n",
    "\n",
    "df = df[df['Nature mutation'] == 'Vente']\n",
    "df = df[df['Code departement'] == 75]\n",
    "df = df[df['Type local'] == 'Appartement']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changement du nom des variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\n",
    "        'Date mutation', 'Valeur fonciere', 'No voie', \n",
    "        'Type de voie', 'Voie', 'Code postal', 'Surface reelle bati', \n",
    "        'Nombre pieces principales',\n",
    "        'Code departement', 'Code commune', 'Code voie',\n",
    "        'Type local', 'Section', 'No plan'\n",
    "        ]]\n",
    "\n",
    "df = df.rename(columns = {\n",
    "                            'Date mutation': 'Date', \n",
    "                            'Valeur fonciere': 'Valeur',\n",
    "                            'Type de voie': 'TypeVoie',\n",
    "                            'No voie': 'Numero',\n",
    "                            'Code postal': 'CodePostal',\n",
    "                            'Surface reelle bati': 'Surface',\n",
    "                            'Nombre pieces principales': 'NbPieces',\n",
    "                            'Surface terrain': 'SurfaceTerrain',\n",
    "                            'Code departement': 'Code_departement', \n",
    "                            'Code commune': 'Code_commune', \n",
    "                            'Code voie': 'Code_voie',\n",
    "                            'Type local': 'TypeLocal',\n",
    "                            'No plan': 'No_plan'\n",
    "                         }\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.2. Gestion des variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.2.1 Variables existantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que la variable \"Valeur\" n'est pas de type float ou int, il va falloir y remédier !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les biens dont la valeur n'est pas renseignée ne nous intéressent pas\n",
    "\n",
    "df = df.dropna(subset = ['Valeur'])\n",
    "\n",
    "# Changement des virgules en point pour pouvoir convertir les types str en types float\n",
    "\n",
    "df['Valeur'] = df.apply(lambda row : str(row.Valeur).replace(',', '.'), axis = 1)\n",
    "df['Valeur'] = pd.to_numeric(df['Valeur'])\n",
    "\n",
    "# On divise par 1000 les prix pour plus de lisibilité\n",
    "\n",
    "df['Valeur'] = df['Valeur'] / 1000\n",
    "\n",
    "# On se concentre sur une tranche de prix \"raisonnable\"\n",
    "\n",
    "df = df[df['Valeur'] > 60]\n",
    "df = df[df['Valeur'] < 15000]\n",
    "\n",
    "# Pour déterminer ces bornes, nous sommes allé sur des sites d'immobiliers à Paris pour \n",
    "# trouver les valeurs extrêmes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Surface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'après la loi, un logement mis en location doit respecter une surface minimum. Il s'agit d'un des critères de décence du logement.\n",
    "\n",
    "Le logement doit comporter au moins une pièce principale présentant :\n",
    "\n",
    "une surface habitable de 9 m² et une hauteur sous plafond minimale de 2,20 mètres,\n",
    "ou un volume habitable de 20 m³."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On trie en conséquence\n",
    "\n",
    "df = df[df['Surface'] >= 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.sort_values('Date')[['Date']].head())\n",
    "df[pd.isna(df['Date'])].shape\n",
    "\n",
    "# Il ne semble pas y avoir de valeurs manquantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elements d'adresse (Numero, TypeVoie,  Voie, CodePostal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Numero\n",
    "\n",
    "print(df[pd.isna(df['Numero'])].shape)\n",
    "\n",
    "# Tous les appartments ont un numéro indiqué. On est satisfait car on souhaite une localisation précise des appartements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TypeVoie\n",
    "\n",
    "print(df[pd.isna(df['TypeVoie'])].shape)\n",
    "\n",
    "# 15 valeurs manquantes, on regarde à quoi elles correspondent\n",
    "\n",
    "display(df[pd.isna(df['TypeVoie'])].sample(3))\n",
    "\n",
    "# Cela correspond à des adresses où le type de voie est spécial (Villa, Pont, Autoroute, Rond point) \n",
    "# et est contenu dans la variable Voie\n",
    "# On laisse comme ca pour l'instant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Voie\n",
    "\n",
    "display(df.sort_values('Voie')[['Voie']].head())\n",
    "print(df[pd.isna(df['Voie'])].shape)\n",
    "\n",
    "# Il ne semble pas y avoir de valeurs manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CodePostal\n",
    "\n",
    "print(df[pd.isna(df['CodePostal'])].shape)\n",
    "\n",
    "# 1 valeur manquante, on regarde à quoi elle correspond\n",
    "\n",
    "display(df[pd.isna(df['CodePostal'])])\n",
    "\n",
    "# La rue de l'Abbé Groult se situe dans le 15ème, on remplit à la main.\n",
    "\n",
    "df.at[2503844, 'CodePostal'] = float(75015)\n",
    "\n",
    "display(df.loc[[2503844]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NbPieces (nombre de pièces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(df[pd.isna(df['NbPieces'])].shape)\n",
    "\n",
    "display(df.sort_values('NbPieces')[['NbPieces']].head(3))\n",
    "\n",
    "# Pas de valeurs manquantes mais des appartements à 0 pièces...\n",
    "\n",
    "display(df[df['NbPieces'] == 0].head(3))\n",
    "\n",
    "# Les entrées n'ont pas l'air des anomalies, la valeur doit être manquante.\n",
    "# On laisse tel quel en gardant à l'esprit cette observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.2.2 Création de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prix au m2\n",
    "\n",
    "df['prixm2'] = df['Valeur'] / df['Surface']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de la variable 'id' qui permettra d'identifier l'adresse précise dans une autre bdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_numero(numero):\n",
    "    \"\"\"Créé le code utilisé pour identifier le numero d'une rue au sein du code 'id' \n",
    "\n",
    "    Argument :\n",
    "    numero : float\n",
    "        numero de la rue tel que présent dans la base de données\n",
    "\n",
    "    Return :\n",
    "    code : str\n",
    "        Le code correspondant (format 00007 pour le numéro 7 d'une rue, par exemple)\n",
    "    \"\"\"\n",
    "\n",
    "    code = str(int(numero))\n",
    "    code = '0' * (5 - len(code)) + code\n",
    "\n",
    "    return code\n",
    "\n",
    "df['id'] = df['Code_departement'].astype(str) + df['Code_commune'].astype(str) + '_' + df['Code_voie'].astype(str) + '_' + df['Numero'].apply(code_numero)\n",
    "\n",
    "df = df.drop(columns = ['Code_commune', 'Code_departement', 'Code_voie'])\n",
    "\n",
    "# Vérification\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Petite sauvegadre de la base à ce stade\n",
    "\n",
    "dfv0 = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.3. Choix de la variable à prédir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# On peut tracer le prix en fonction de la surface\n",
    "\n",
    "df[df['Valeur'] < 2000].sample(2000).plot(x = 'Surface', y = 'Valeur', kind = 'scatter', alpha = .1)\n",
    "\n",
    "# On a tracé le graphe pour les appartements ayant un prix inférieur à 2 millions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Valeur'] < 2000]['Valeur'].corr(df[df['Valeur'] < 2000]['Surface'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit une relation linéaire apparaitre, justifiée par un coefficient de corrélation significatif, de 0.72 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On fait donc l'hypothèse qu'il y a une corélation linéaire entre le prix d'un appartement et sa surface en m2.\n",
    "#### On se donnera donc pour objectif de prédir le prix au m2 d'un appartement en fonction de différents paramètres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.4. Gestion des anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La base de données n'est pas parfaite, certaines entrées sont erronées.\n",
    "La premier nettoyage que nous allons faire, qui est aussi le plus grossier, est de se restreindre à des prix au m2 raisonnables.  \n",
    "Le site meilleursagents.com recense les prix au m2 de nombreux appartements à Paris.  \n",
    "Les prix les plus bas observé sont environ de 4 700 €/m2  \n",
    "Les prix les plus hauts observés sont environ de 32 000 €/m2\n",
    "\n",
    "#### On choisit donc de considérer les biens dont le prix au m2 est compris entre 4 230 € et 35 200 (marge de 10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélection de la tranche de prix choisie\n",
    "\n",
    "df = df[df['prixm2'] > 4.23]\n",
    "df = df[df['prixm2'] < 35.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(x = 'Surface', y = 'Valeur', kind = 'scatter', alpha = .1)\n",
    "\n",
    "df['Valeur'].corr(df['Surface'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On obtiens donc un \"cône\" de données, et le coefficient de corrélation n'en est que renforcé : 0.87"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Enrichissement de la base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.1 Couplage : date de construction du bâtis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour obtenir la date de construction des immeubles, on utilise deux nouvelles bases :  \n",
    "  \n",
    "   -df_dates qui donne la date de construction à partir d'un identificateur id2  \n",
    "   -df_join qui associe l'identificateur id2 à une adresse\n",
    "    \n",
    "   df_join va donc nous permettre de faire la jointure entre la base d'origine et la base df_dates pour obtenir la date deconstruction des immeubles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des bases\n",
    "\n",
    "df_dates = pd.read_csv('data/date_construction.csv', sep = ',')\n",
    "df_join = pd.read_csv('data/PARCELLE_CADASTRALE.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On garde que les variables qui nous sont utiles ici\n",
    "\n",
    "df_join = df_join[['n_sq_pc', 'c_sec', 'n_pc']]\n",
    "df_dates = df_dates[['n_sq_pc', 'c_perconst']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On regroupe les deux bases grâce à l'identificateur\n",
    "\n",
    "df_dates = df_dates.merge(df_join, left_on = 'n_sq_pc', right_on = 'n_sq_pc')\n",
    "df_dates = df_dates.drop(columns = ['n_sq_pc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On crée l'identificateur dans la base d'origine et le fait correspondre avec celui de la base des dates\n",
    "\n",
    "df['id2'] = df.apply(lambda row: str(row.Section) + str(row.No_plan), axis = 1)\n",
    "df_dates['id2'] = df_dates.apply(lambda row: str(row.c_sec) + str(row.n_pc), axis = 1)\n",
    "df_dates = df_dates.drop_duplicates(subset = ['id2'], keep = 'first')\n",
    "\n",
    "display(df_dates.head(3))\n",
    "display(df.head(3))\n",
    "\n",
    "\n",
    "# On joint les deux bases\n",
    "\n",
    "df = pd.merge(df, df_dates, on = 'id2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Et on nettoie le tout\n",
    "\n",
    "df = df.drop(columns = ['id2', 'Section', 'No_plan'])\n",
    "df = df.rename(columns = {'c_perconst': 'periode_construction'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(df.sample(10000), x = 'periode_construction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La catégorie 4 n'existe pas, on recode pour que ca soit plus compréhensif\n",
    "\n",
    "def ajustement(valeur):\n",
    "    \n",
    "    if valeur in [i for i in range(5, 13)]:\n",
    "        return valeur - 1\n",
    "    \n",
    "    elif valeur == 99:\n",
    "        return 0\n",
    "    \n",
    "    else:\n",
    "        return valeur\n",
    "\n",
    "df['periode_construction'] = df.apply(lambda row: ajustement(row.periode_construction), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(df.sample(1000), x = 'periode_construction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On obtient ici plusieurs tranches de périodes de constructions.\n",
    "D'après la documentation de la base :  \n",
    "0 : Données manquantes  \n",
    "1 : Avant 1800\t\n",
    "2 : 1801-1850\t\n",
    "3 : 1851-1914\t\n",
    "4 : 1915-1939\t\n",
    "5 : 1940-1967\t\n",
    "6 : 1968-1975\t\n",
    "7 : 1976-1981\t\n",
    "8 : 1982-1989\t\n",
    "9 : 1990-1999\t\n",
    "10 : 2000-2007\t\n",
    "11 : 2008 et plus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *A partir de maintenant, les exemples seront faits sur une petite portion de la base, pour éviter les calculs trop longs.*\n",
    "### *La base a été traitée en entier par nos soins en amont, puis sauvegardée*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(100)\n",
    "df_traitee = pd.read_csv('data/données_calculees.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.2 Ajout de la variable arrondissement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Arrondissement'] = df.apply(lambda row: int(str(int(row.CodePostal))[3:5]), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.3 Couplage avec la base de données GPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On souhaite obtenir les coordonnées GPS de tous les appartements présents dans la base.\n",
    "Pour cela, nous avons trouvé une base de données qui répertorie toutes les adresses parisiennes et leur associe des coordonnées GPS.\n",
    "Pour coupler les deux bases, nous utiliserons la variable id précedemment crée.\n",
    "Ce code id est de la forme WWXXX_YYYY_ZZZZZ avec : \t\n",
    "##### WW est le code département (75 ici)\n",
    "##### XXX est le code commune (115 pour le 15ème arrondissement)\n",
    "##### YYYY est le code voie (4903 pour l'avenue Jean Jaurès par exemple)\n",
    "##### ZZZZ est le numéro (00005 pour le 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('data/gps.csv', sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[df2['id'].str[:2] == '75'].sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Séléction des variables utiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_GPS = df2[[\n",
    "        'id', 'lon', 'lat', \n",
    "        ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Couplage avec df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.merge(df_GPS, left_on = 'id', right_on = 'id')\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut donc maintenant représenter les appartements dans un plan de Paris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_osm = folium.Map(location = [48.853332, 2.348776], tiles = 'CartoDB dark_matter', zoom_start = 12.2)\n",
    "\n",
    "df_print = df_traitee.sample(1000)\n",
    "\n",
    "paliers = np.linspace(min(df_print['prixm2']), max(df_print['prixm2']), 10)\n",
    "\n",
    "for i in range(9):\n",
    "    df_pal = df_print[df_print['prixm2'] < paliers[i + 1]]\n",
    "    df_pal = df_print[df_print['prixm2'] > paliers[i]]\n",
    "    couleur = 'rgb(' + str(255) + ',' + str(255 * (10 - i) / 10) + ',' + str(0) + ')'\n",
    "\n",
    "    for lat, lon in [(row.lat, row.lon) for _, row in df_pal.iterrows()]:\n",
    "        map_osm.add_child(folium.CircleMarker(location = [lat, lon], radius = 1, color = couleur))\n",
    "    \n",
    "map_osm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(\n",
    "        df_traitee, \n",
    "        x = 'lon', \n",
    "        y = 'lat', \n",
    "        color = 'prixm2', \n",
    "        template = 'none', \n",
    "        color_continuous_scale = 'Jet', \n",
    "        opacity = .2\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate bien les faits stylisé déjà connu : \n",
    "#### Les quartiers périphériques sont en général moins chers que les quartiers centraux.\n",
    "#### Les quartiers de l'ouest sont plus chers que ceux à l'est."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.4 Ajout de la variable du Quartier administatif\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation de la base des quartiers\n",
    "\n",
    "df_quartier = pd.read_csv('data/quartier_paris.csv', sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélection des variables utiles\n",
    "\n",
    "df_quartier = df_quartier[['c_qu', 'geom']]\n",
    "\n",
    "# Traitement avec geopandas\n",
    "\n",
    "df_quartier['geom'] = df_quartier.apply(lambda row: json.loads(row.geom), axis = 1)\n",
    "geom = [shape(i) for i in df_quartier['geom']]\n",
    "df_quartier['geom'] = gpd.GeoDataFrame({'geometry':geom})['geometry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_quartier(lat, lon):\n",
    "    \"\"\"\n",
    "    Cette fonction détermine le quartier administratif d'un appartement en fonction de ses\n",
    "    coordonnées GPS\n",
    "    \"\"\"\n",
    "    \n",
    "    for _, row in df_quartier.iterrows():\n",
    "        \n",
    "        polygon = row.geom\n",
    "        \n",
    "        if polygon.contains(Point(lon, lat)):\n",
    "            return row.c_qu\n",
    "        \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout de la variable\n",
    "\n",
    "df['Quartier'] = df.apply(lambda row: determine_quartier(row.lat, row.lon), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.5 Ajout du score de proximité aux espaces verts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation de la base des espaces verts\n",
    "\n",
    "df_jardin = pd.read_csv('data/espaces_verts.csv', sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélection des catégories utiles\n",
    "\n",
    "df_jardin = df_jardin[df_jardin['Catégorie'].isin(['Square', 'Jardin', 'Pelouse', 'Parc', 'Bois'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On renome les variables\n",
    "\n",
    "df_jardin = df_jardin[[\n",
    "        'Superficie totale réelle', 'Geo Shape', \n",
    "        '''Nom de l'espace vert''']].rename(columns = {\n",
    "                                        'Superficie totale réelle' : 'Aire',\n",
    "                                        'Geo Shape' : 'geom',\n",
    "                                        '''Nom de l'espace vert''' : 'Nom'\n",
    "                                                        }\n",
    "                                            )\n",
    "df_jardin = df_jardin.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traitement avec geoapndas\n",
    "\n",
    "df_jardin['geom'] = df_jardin.apply(lambda row: json.loads(str(row.geom)), axis = 1)\n",
    "geom = [shape(i) for i in df_jardin['geom']]\n",
    "df_jardin['geom'] = gpd.GeoDataFrame({'geometry':geom})['geometry']\n",
    "df_jardin = df_jardin.dropna(subset=['Aire'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardisation des aires des jardins\n",
    "\n",
    "pt = preprocessing.PowerTransformer(method='box-cox', standardize = False)\n",
    "df_jardin[['Aire']] = pt.fit_transform(df_jardin[['Aire']])\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "df_jardin[['Aire']] = min_max_scaler.fit_transform(df_jardin[['Aire']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcule_score_jardin(lat, lon):\n",
    "    \"\"\"\n",
    "    Le score se calcule ainsi : score = sqrt(aire du jardin le plus proche) / distance à ce jardin\n",
    "    \"\"\"\n",
    "    L = []\n",
    "    for _, row in df_jardin.iterrows():\n",
    "\n",
    "        polygon = row.geom\n",
    "\n",
    "        distance = polygon.distance(Point(lon, lat))\n",
    "        L.append((distance, row.Aire))\n",
    "        \n",
    "    L = sorted(L, key = lambda x:  x[0])\n",
    "    (distance, aire) = L[0]\n",
    "    score = np.sqrt(aire) / (distance * 111) # On multiplie par 111 pour convertir la distance en degré en metres\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['score_jardin'] = df.apply(lambda row: calcule_score_jardin(row.lat, row.lon), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.6 Ajout du score de proximité aux monuments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation de la base des monuments\n",
    "\n",
    "df_monuments = pd.read_csv('data/monuments.csv', sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On standardise le nombre de visiteurs\n",
    "\n",
    "df_monuments['visiteurs']/=max(df_monuments['visiteurs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcul du score de proximité aux monuments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcule_scores_monument(lat,lon):\n",
    "\n",
    "    distances_aux_monuments = []\n",
    "\n",
    "    for i,row in df_monuments.iterrows():\n",
    "        distances_aux_monuments.append((row.nom, row.visiteurs, vincenty((row.lat, row.lon), (lat, lon))))\n",
    "\n",
    "    try:\n",
    "        score_1 = max([monument[1] for monument in distances_aux_monuments if monument[2] < 1])\n",
    "    except:\n",
    "        score_1 = 0\n",
    "\n",
    "    try:\n",
    "        score_2 = sum([monument[1]/monument[2] for monument in distances_aux_monuments if monument[2] < 3])\n",
    "    except:\n",
    "        score_2 = 0\n",
    "\n",
    "    return (score_1, score_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des scores\n",
    "\n",
    "L1, L2 = [], []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    (a,b) = calcule_scores_monument(row.lat, row.lon)\n",
    "    L1.append(a)\n",
    "    L2.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul du score final\n",
    "\n",
    "df['score_1'], df['score_2'] = L1, L2\n",
    "df['score_2'] /= max(df['score_2'])\n",
    "df['score_monument'] = ( (1/5) * df['score_1'] + (4/5) * df['score_2'] ) \n",
    "df = df.drop(columns = ['score_1', 'score_2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.7 Ajout du score de proximité aux stations de métro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.7.1 Traitement de la base des stations parisiennes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importation de la base\n",
    "\n",
    "df_metro = pd.read_csv('data/stations_metro.csv', sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On garde les colonnes et les lignes qui nous intéressent\n",
    "\n",
    "df_metro = df_metro[[\n",
    "        'Geo Point', 'nomlong', 'reseau',\n",
    "                    ]]\n",
    "\n",
    "df_metro = df_metro.where(df_metro['reseau'] == 'METRO').dropna()\n",
    "df_metro.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problème, la variable Geo Point doit être traitée pour obtenir la latitude et la longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fonctions de traitement des coordonnées \n",
    "\n",
    "def sepvirgulex(s):\n",
    "    i = 0\n",
    "    while s[i] != ',':\n",
    "        i+=1\n",
    "        \n",
    "    x = float(s[:i])\n",
    "    \n",
    "    return x\n",
    "\n",
    "def sepvirguley(s):\n",
    "    i = 0\n",
    "    while s[i] != ',':\n",
    "        i+=1\n",
    "        \n",
    "    y = float(s[i+1:])\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crétaion des variables de lattitude et longitude à partir de la variables des coordonnées\n",
    "\n",
    "df_metro['lat'] = df_metro['Geo Point'].apply(sepvirgulex)\n",
    "df_metro['lon'] = df_metro['Geo Point'].apply(sepvirguley)\n",
    "\n",
    "df_metro = df_metro[['nomlong','reseau','lon','lat']].rename(columns = {\n",
    "                                                            'nomlong': 'Station', \n",
    "                                                                         }\n",
    "                                                            )\n",
    "\n",
    "df_metro.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.7.1 Traitement de la base donnant la frequentation des stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importation de la base \n",
    "\n",
    "df_metro_pop = pd.read_csv('data/trafic_metro.csv', sep = \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On retient les colonnes et les lignes qui nous intéressent\n",
    "\n",
    "df_metro_pop = df_metro_pop[['Rang','Station','Réseau','Trafic']].rename(columns = {'Réseau' : 'Reseau'})\n",
    "df_metro_pop = df_metro_pop.where(df_metro_pop['Reseau'] == 'Métro').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traitement du nom des stations pour qu'ils collent entre les deux bases\n",
    "\n",
    "def sansparenthese(s):\n",
    "    for i in range(len(s)):\n",
    "        if s[i] == '(':\n",
    "            \n",
    "            return s[:i]\n",
    "        \n",
    "    return s\n",
    "\n",
    "def sanstiret(s):\n",
    "    for i in range(len(s)):\n",
    "        if s[i]=='-':\n",
    "            s = s[:i] + ' ' + s[i+1:]\n",
    "            \n",
    "    return s\n",
    "\n",
    "def sansespace(s):\n",
    "    if s[-1] != ' ':\n",
    "        \n",
    "        return s\n",
    "    else:\n",
    "        \n",
    "        return sansespace(s[:-1])\n",
    "\n",
    "def sansrer(s):\n",
    "    if s[-4:]== ' RER':\n",
    "        return s[:-4]\n",
    "    \n",
    "    return s\n",
    "\n",
    "df_metro['Station'] = df_metro['Station'].apply(sanstiret)\n",
    "df_metro['Station'] = df_metro['Station'].apply(sansparenthese)\n",
    "df_metro['Station'] = df_metro['Station'].apply(sansespace)\n",
    "df_metro['Station'] = df_metro['Station'].apply(sansrer)\n",
    "\n",
    "df_metro = df_metro.replace([\n",
    "    'LES COURTILLES ASNIERES GENNEVILLIERS', 'BOBIGNY PANTIN', 'SAINT MANDE', 'LOUVRE RIVOLI', \n",
    "    'MALAKOFF RUE ÉTIENNE DOLET', 'LA DEFENSE GRANDE ARCHE', 'BIBLIOTHEQUE FRANCOIS MITTERRAND', \n",
    "    'LES AGNETTES ASNIERES GENNEVILLIERS', 'PALAIS ROYAL MUSEE DU LOUVRE', \"CHAUSSEE D'ANTIN\", \n",
    "    'AUBERVILLIERS PANTIN', 'PLACE DE CLICHY', 'PEREIRE LEVALLOIS', 'JAVEL', 'MONTPARNASSE', \n",
    "    'GABRIEL PERI ASNIERES GENNEVILLIERS', 'FRANKLIN D.ROOSEVELT', 'AUSTERLITZ'\n",
    "                            ], \n",
    "                           [\n",
    "    'LES COURTILLES', 'BOBIGNY PANTIN RAYMOND QUENEAU', 'SAINT MANDE TOURELLE', 'LOUVRE', \n",
    "    'MALAKOFF RUE ETIENNE DOLET', 'LA DEFENSE', 'BIBLIOTHEQUE',\n",
    "    'LES AGNETTES', 'PALAIS ROYAL', \"CHAUSSEE D'ANTIN LA FAYETTE\", 'AUBERVILLIERS PANTIN QUATRE CHEMINS',\n",
    "    'PLACE CLICHY', 'PEREIRE', 'JAVEL ANDRE CITROEN', 'MONTPARNASSE BIENVENUE', 'GABRIEL PERI',\n",
    "    'FRANKLIN D. ROOSEVELT', \"GARE D'AUSTERLITZ\"\n",
    "                            ])\n",
    "\n",
    "df_metro_pop = df_metro_pop.replace(['Métro'],['Metro'])\n",
    "df_metro_pop['Station'] = df_metro_pop['Station'].apply(sanstiret)\n",
    "df_metro_pop['Station'] = df_metro_pop['Station'].apply(sansparenthese)\n",
    "df_metro_pop['Station'] = df_metro_pop['Station'].apply(sansespace)\n",
    "df_metro_pop['Station'] = df_metro_pop['Station'].apply(sansrer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On standardise la féréquentation\n",
    "\n",
    "df_metro_pop['Trafic'] /= max(df_metro_pop['Trafic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut maintenant calculer le score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcule_scores_station(lon,lat):\n",
    "\n",
    "    distances_aux_stations = []\n",
    "\n",
    "    for i,row in df_metro.iterrows():\n",
    "        nom = df_metro.loc[i]['Station']\n",
    "        popularite = max(df_metro_pop['Trafic'].where(df_metro_pop['Station'] == nom).dropna())\n",
    "        distances_aux_stations.append((row.Station, popularite, vincenty((row.lat, row.lon), (lat, lon))))\n",
    "\n",
    "    try:\n",
    "        score_1 = max([station[1] for station in distances_aux_stations if station[2] < 0.2])\n",
    "    except:\n",
    "        score_1 = 0\n",
    "\n",
    "    try:\n",
    "        score_2 = max([station[1] for station in distances_aux_stations if station[2] < 0.7])\n",
    "    except:\n",
    "        score_2 = 0\n",
    "\n",
    "    return score_1, score_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calcul des deux scores\n",
    "\n",
    "L1, L2 = [], []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    (a,b) = calcule_scores_station(row.lon, row.lat)\n",
    "    L1.append(a)\n",
    "    L2.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul du score final\n",
    "\n",
    "df['score_1'], df['score_2'] = L1, L2\n",
    "\n",
    "df['score_metro'] = ( (1/5) * df['score_1'] + (4/5) * df['score_2'] ) \n",
    "\n",
    "df = df.drop(columns = ['score_1', 'score_2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.8 Ajout des scores de proximité aux commerces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation de la base des commerces\n",
    "\n",
    "df_commerce = pd.read_csv('data/commerces.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On garde les lignes et colonnes qui nous intéressent\n",
    "\n",
    "df_commerce = df_commerce[['ARRO', 'NUM', 'TYP_VOIE', 'LIB_VOIE', 'LIBACT' ]]\n",
    "df_commerce.dropna()\n",
    "df_commerce.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adri(i):\n",
    "    \n",
    "    \"\"\"\n",
    "    A partir de la base de données, renvoie l'adresse selon les normes françaises\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(str(df_commerce.loc[i]['ARRO']))==2:\n",
    "        adr = str(df_commerce.loc[i]['NUM']) + ', ' + str(df_commerce.loc[0]['TYP_VOIE']) + ' '+ str(df_commerce.loc[i]['LIB_VOIE']) + ', 750' + str(df_commerce.loc[i]['ARRO']) + ' Paris'\n",
    "    \n",
    "    else:\n",
    "        adr = str(df_commerce.loc[i]['NUM']) + ', ' + str(df_commerce.loc[0]['TYP_VOIE']) + ' '+ str(df_commerce.loc[i]['LIB_VOIE']) + ', 7500' + str(df_commerce.loc[i]['ARRO']) + ' Paris'\n",
    "    \n",
    "    return adr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fonction nous permet d'associer, à chaque commerce, ses coorodonnées gps.\n",
    "On a fait tourné cette fonction pour obtenir la base suivante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_commerce = pd.read_csv('data/commerces_gps.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation de la base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste de la population par arrondissement\n",
    "\n",
    "pop_par_ar = [\n",
    "              16252,20260,34788,27487,59108,40916,52512,36453,59269,91932,\n",
    "              147017,141494,181552,137105,233484,165446,167835,195060,186393,195604\n",
    "             ]\n",
    "\n",
    "\n",
    "df_com_arro = df_commerce['ARRO'].value_counts()\n",
    "X = [str(i) for i in range(1,21)]\n",
    "Y = [df_com_arro[i+1]/pop_par_ar[i] for i in range(20)]\n",
    "plt.bar(X,Y)\n",
    "plt.xlabel('Arrondissement')\n",
    "plt.title('Nombre de commerces par habitant selon les arrondissements')\n",
    "plt.show()\n",
    "\n",
    "#On remarque que des arrondissements comme le 1er ou le 4ème ont beaucoup de commerces par habitant, au contraire du 13ème qui lui possède  une part de comemrces par habitant plus faible\"\"\"\n",
    "\n",
    "#source INSEE, qu'on peut aussi lire ici https://www.apur.org/sites/default/files/documents/recueil_thematique_1234_arr_paris_0.pdf\n",
    "#on se donne les surfaces des arrondissements en hectare\n",
    "\n",
    "# Liste de la surface par arrondissement\n",
    "\n",
    "surf_arrond =[182,99,117,160,254,215,409,388,218,289,367,639,715,562,850,790,567,600,679,598]\n",
    "sum(surf_arrond)\n",
    "\n",
    "Xs = [str(i) for i in range(1,21)]\n",
    "Ys= [df_com_arro[i+1]/surf_arrond[i] for i in range(20)]\n",
    "\n",
    "plt.bar(Xs,Ys)\n",
    "plt.xlabel('Arrondissement')\n",
    "plt.title('Nombre de commerces par hectare selon les arrondissements')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a alors un problème : on ne peut pas construire un indice comme précedemment, car cela voudrait dire \n",
    "calculer la distance de chaque appartement à chaque commerce. Si cela a été possible pour les espaces verts, \n",
    "pour les stations de métro, cela semble beaucoup trop long pour les commerces, qui sont vraiment plus nombreux.  \n",
    "Pour palier à ce problème, nous quadrillons Paris en 29 x 29 zones, nous calculons le nombre de commerces dans chaque zone, et on associe à chaque appartement le nombre de commerces de la zone où il se situe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 29 # Taille du quadrillage\n",
    "\n",
    "max_lon = max(df_commerce['lon'])\n",
    "min_lon = min(df_commerce['lon'])\n",
    "max_lat = 48.900391\n",
    "min_lat = min(df_commerce['lat'])\n",
    "\n",
    "linsplon = list(np.linspace(min_lon * (1 - 10 ** -7),max_lon*(1 + 10 ** -7), N))\n",
    "linsplat = list(np.linspace(min_lat * (1 -10 ** -7),max_lat*(1 + 10 ** -7), N))\n",
    "\n",
    "# Fonction qui renvoie l'indice de la première occurence de la liste \n",
    "# triée qui est strictement plus grande que l'argument x\n",
    "\n",
    "def premier_plus_grand(L,x): \n",
    "    i=0\n",
    "    \n",
    "    while L[i]<= x:\n",
    "        i+=1\n",
    "        \n",
    "    return i\n",
    "\n",
    "def donne_zone(lat, lon, Linsplat, Linsplon):\n",
    "  \n",
    "    i = premier_plus_grand(Linsplat,lat)\n",
    "    j = premier_plus_grand(Linsplon,lon)\n",
    "    \n",
    "    return i,j\n",
    "\n",
    "zones_tout_commerce = np.zeros((N,N))\n",
    "zones_commerce_luxe = np.zeros((N,N))\n",
    "\n",
    "for k in range(len(df_commerce['lat'])):\n",
    "    \n",
    "    i,j = donne_zone(df_commerce['lat'][k], df_commerce['lon'][k], linsplat, linsplon)\n",
    "    zones_tout_commerce[i][j] += 1\n",
    "    \n",
    "df_commerce_luxe = df_commerce[df_commerce['LIBACT'].isin(['Commerce détail de boissons', \n",
    "    'Charcuterie - Traiteur - Epicerie fine', 'Chocolaterie - Confiserie', \n",
    "    'Produits alimentaires bio et circuits courts', 'Crèmerie - Fromagerie', \n",
    "    'Torréfacteur - Commerce détail thé et café', 'Poisonnerie', \n",
    "    'Glacier : vente à emporter et consommation sur place', 'Alimentation générale de luxe > 300m²'\n",
    "                                                            ]\n",
    "                                                         )\n",
    "                              ].reset_index(drop = True)\n",
    "    \n",
    "for k in range(len(df_commerce_luxe['lat'])):\n",
    "    i,j = donne_zone(df_commerce_luxe['lat'][k], df_commerce_luxe['lon'][k], linsplat, linsplon)\n",
    "    zones_commerce_luxe[i][j] += 1\n",
    "\n",
    "def Zone_appart(lat,lon):\n",
    "    i,j = donne_zone(lat,lon, linsplat, linsplon)\n",
    "    return zones[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcule_scores_commerces(lat,lon):\n",
    "    \n",
    "    i,j = donne_zone(lat, lon, linsplat, linsplon)\n",
    "    score_1, score_2 = zones_tout_commerce[i][j], zones_commerce_luxe[i][j]\n",
    "    \n",
    "    return score_1, score_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des scores\n",
    "\n",
    "L1, L2 = [], []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    (a,b) = calcule_scores_commerces(row.lat, row.lon)\n",
    "    L1.append(a)\n",
    "    L2.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integration à la base de données\n",
    "\n",
    "df['score_commerce'], df['score_commerce_lux'] = L1, L2 \n",
    "\n",
    "df['score_commerce'] /= max(df['score_commerce'])\n",
    "df['score_commerce_lux'] /= max(df['score_commerce_lux'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = {'Date','Valeur', 'Voie', 'Surface', 'TypeLocal', 'id', 'c_sec', 'n_pc'})\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**On obtient ici une base de données enrichie de plusieurs variables, qui pourra être exploitée par la suite.  \n",
    "La suite au prochain notebook!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
